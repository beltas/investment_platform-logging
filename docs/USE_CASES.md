# Agora Logging Library - Use Cases

**Version:** 1.0  
**Date:** January 2025  
**Status:** Final

This document describes the primary use cases, scenarios, and user stories for the Agora Trading Platform logging library across all supported languages.

---

## Table of Contents

1. [Primary Use Cases by Language](#primary-use-cases-by-language)
   - [Python FastAPI Services](#python-fastapi-services)
   - [C++ gRPC Services](#c-grpc-services)
   - [Node.js NestJS Services](#nodejs-nestjs-services)
2. [Context Propagation Scenarios](#context-propagation-scenarios)
3. [Log Rotation Scenarios](#log-rotation-scenarios)
4. [Error Handling and Graceful Degradation](#error-handling-and-graceful-degradation)
5. [Performance-Critical Logging](#performance-critical-logging)
6. [Distributed Tracing Integration](#distributed-tracing-integration)

---

## Primary Use Cases by Language

### Python FastAPI Services

The Python logging implementation is used by Market Data Service, Analysis Engine, Recommendation Engine, and Time Series Analysis Service.

#### UC-PY-001: Service Initialization

**User Story:** As a Python service developer, I want to initialize logging once at startup so that all subsequent log calls include service metadata automatically.

**Scenario:**
```python
# main.py - Application startup
from fastapi import FastAPI
from agora_log import initialize, LogConfig, get_logger

# Initialize once at startup
config = LogConfig(
    service_name="market-data-service",
    environment="production",
    version="1.2.3",
    file_path="/var/log/agora/market-data.log",
    max_file_size_mb=100,
    max_backup_count=5
)
initialize(config)

app = FastAPI()
logger = get_logger("agora.market_data")

@app.on_event("startup")
async def startup():
    logger.info("Service started", port=8001)
```

**Expected Output (JSON file):**
```json
{
  "timestamp": "2025-01-15T10:30:45.123456Z",
  "level": "INFO",
  "message": "Service started",
  "service": "market-data-service",
  "environment": "production",
  "version": "1.2.3",
  "host": "market-data-pod-abc123",
  "logger_name": "agora.market_data",
  "file": "main.py",
  "line": 18,
  "function": "startup",
  "context": {
    "port": 8001
  }
}
```

#### UC-PY-002: Request-Scoped Logging with Correlation ID

**User Story:** As a developer, I want each HTTP request to have a unique correlation ID so that I can trace the entire request lifecycle across log entries.

**Scenario:**
```python
from fastapi import FastAPI, Request
from agora_log import get_logger
from agora_log.integrations.fastapi import LoggingMiddleware, get_request_logger
import uuid

app = FastAPI()
app.add_middleware(LoggingMiddleware)

@app.get("/prices/{symbol}")
async def get_price(symbol: str, request: Request):
    # Logger automatically has correlation_id from middleware
    logger = get_request_logger()
    
    logger.info("Fetching price", symbol=symbol)
    
    try:
        price = await fetch_from_database(symbol)
        logger.info("Price fetched", symbol=symbol, price=price)
        return {"symbol": symbol, "price": price}
    except Exception as e:
        logger.error("Failed to fetch price", exception=e, symbol=symbol)
        raise
```

**Expected Behavior:**
- All log entries within the same request share the same `correlation_id`
- The correlation ID is automatically generated by middleware
- If `X-Correlation-ID` header is present, it is reused

#### UC-PY-003: Database Operation Timing

**User Story:** As an operations engineer, I want to automatically log the duration of database queries so that I can identify slow queries in production.

**Scenario:**
```python
async def get_historical_prices(symbol: str, start_date: date, end_date: date):
    logger = get_request_logger()
    
    with logger.timer("Database query: historical prices"):
        query = """
            SELECT time, open, high, low, close, volume
            FROM stock_prices
            WHERE symbol = $1 AND time BETWEEN $2 AND $3
            ORDER BY time
        """
        rows = await db.fetch(query, symbol, start_date, end_date)
    
    logger.info("Historical prices retrieved", 
                symbol=symbol, 
                row_count=len(rows))
    return rows
```

**Expected Output:**
```json
{
  "timestamp": "2025-01-15T10:30:45.250000Z",
  "level": "INFO",
  "message": "Database query: historical prices",
  "duration_ms": 125.5,
  "file": "repository.py",
  "line": 45,
  "function": "get_historical_prices"
}
```

#### UC-PY-004: ML Model Prediction Logging

**User Story:** As an ML engineer, I want to log model predictions with metadata so that I can monitor model performance and debug prediction issues.

**Scenario:**
```python
from agora_log import get_logger

logger = get_logger("agora.analysis.predictions")

async def generate_prediction(symbol: str, model_id: str):
    prediction_logger = logger.with_context(
        symbol=symbol,
        model_id=model_id,
        model_version="2.1.0"
    )
    
    prediction_logger.info("Starting prediction")
    
    with prediction_logger.timer("Feature engineering"):
        features = await compute_features(symbol)
    
    with prediction_logger.timer("Model inference"):
        prediction = model.predict(features)
    
    prediction_logger.info("Prediction completed",
        predicted_price=prediction.price,
        confidence=prediction.confidence,
        direction=prediction.direction)
    
    return prediction
```

#### UC-PY-005: Batch Processing Logging

**User Story:** As a data engineer, I want to log progress during batch operations so that I can monitor long-running data ingestion jobs.

**Scenario:**
```python
async def ingest_daily_prices(symbols: list[str]):
    logger = get_logger("agora.market_data.ingestion")
    batch_id = str(uuid.uuid4())
    
    batch_logger = logger.with_context(
        batch_id=batch_id,
        total_symbols=len(symbols)
    )
    
    batch_logger.info("Starting batch ingestion")
    
    success_count = 0
    error_count = 0
    
    for i, symbol in enumerate(symbols):
        symbol_logger = batch_logger.with_context(
            symbol=symbol,
            progress=f"{i+1}/{len(symbols)}"
        )
        
        try:
            await fetch_and_store_price(symbol)
            success_count += 1
            symbol_logger.debug("Symbol processed successfully")
        except Exception as e:
            error_count += 1
            symbol_logger.error("Failed to process symbol", exception=e)
    
    batch_logger.info("Batch ingestion completed",
        success_count=success_count,
        error_count=error_count,
        duration_minutes=elapsed_time.total_seconds() / 60)
```

---

### C++ gRPC Services

The C++ logging implementation is used by the Portfolio Manager service, which requires ultra-low latency.

#### UC-CPP-001: Service Initialization with Environment Configuration

**User Story:** As a C++ developer, I want to initialize logging from environment variables so that I can configure logging without recompiling.

**Scenario:**
```cpp
// main.cpp
#include <agora/log/logger.hpp>
#include <agora/log/config.hpp>

int main() {
    using namespace agora::log;
    
    // Load configuration from environment variables
    auto config_result = Config::from_env("portfolio-manager");
    if (!config_result) {
        std::cerr << "Failed to load log config: " 
                  << config_result.error().message << '\n';
        return 1;
    }
    
    auto init_result = initialize(*config_result);
    if (!init_result) {
        std::cerr << "Failed to initialize logging: "
                  << init_result.error().message << '\n';
        return 1;
    }
    
    auto logger = get_logger("agora.portfolio.main");
    logger.info("Portfolio Manager starting", {
        {"grpc_port", 50052},
        {"rest_port", 8003}
    });
    
    run_server();
    
    shutdown();
    return 0;
}
```

#### UC-CPP-002: gRPC Request Context Extraction

**User Story:** As a C++ developer handling gRPC requests, I want to extract correlation IDs and user context from gRPC metadata so that I can trace requests across services.

**Scenario:**
```cpp
// portfolio_service.cpp
#include <agora/log/logger.hpp>
#include <agora/log/context.hpp>
#include <grpcpp/grpcpp.h>

grpc::Status PortfolioServiceImpl::GetPortfolio(
    grpc::ServerContext* context,
    const GetPortfolioRequest* request,
    Portfolio* response
) {
    auto logger = agora::log::get_logger("agora.portfolio.grpc");
    
    // Extract metadata from gRPC context
    auto metadata = context->client_metadata();
    std::string correlation_id;
    std::string user_id;
    
    if (auto it = metadata.find("x-correlation-id"); it != metadata.end()) {
        correlation_id = std::string(it->second.data(), it->second.size());
    }
    if (auto it = metadata.find("x-user-id"); it != metadata.end()) {
        user_id = std::string(it->second.data(), it->second.size());
    }
    
    // Create request-scoped logger with context
    auto req_logger = logger.with_context(
        agora::log::ctx()
            .correlation_id(correlation_id)
            .user_id(user_id)
            .add("portfolio_id", request->portfolio_id())
            .build()
    );
    
    req_logger.info("Processing GetPortfolio request");
    
    // RAII timer - logs duration on scope exit
    auto timer = req_logger.timer("GetPortfolio completed");
    
    try {
        auto portfolio = repository_.get_portfolio(request->portfolio_id());
        if (!portfolio) {
            req_logger.warning("Portfolio not found");
            return grpc::Status(grpc::StatusCode::NOT_FOUND, "Portfolio not found");
        }
        
        *response = to_proto(*portfolio);
        req_logger.info("Request completed", {
            {"positions_count", static_cast<int64_t>(portfolio->positions.size())},
            {"cash_balance", portfolio->cash_balance}
        });
        
        return grpc::Status::OK;
        
    } catch (const std::exception& ex) {
        timer.cancel(); // Don't log duration on error
        req_logger.error("Request failed", ex);
        return grpc::Status(grpc::StatusCode::INTERNAL, "Internal error");
    }
}
```

#### UC-CPP-003: High-Frequency Transaction Logging

**User Story:** As a performance engineer, I want to log high-frequency transactions without impacting latency so that I can maintain audit trails without sacrificing performance.

**Scenario:**
```cpp
// transaction_processor.cpp
#include <agora/log/logger.hpp>
#include <agora/log/macros.hpp>

void TransactionProcessor::process_transaction(const Transaction& txn) {
    auto logger = agora::log::get_logger("agora.portfolio.transactions");
    
    auto txn_logger = logger.with_context({
        {"transaction_id", txn.id},
        {"portfolio_id", txn.portfolio_id},
        {"symbol", txn.symbol}
    });
    
    // DEBUG logs compile to no-ops in release builds
    // Zero runtime cost when AGORA_LOG_MIN_LEVEL > Debug
    AGORA_LOG_DEBUG(txn_logger, "Processing transaction", {
        {"quantity", txn.quantity},
        {"price", txn.price}
    });
    
    auto result = execute_transaction(txn);
    
    if (result.success) {
        // INFO logs are always compiled in
        txn_logger.info("Transaction executed", {
            {"net_amount", result.net_amount},
            {"new_position", result.new_position}
        });
    } else {
        txn_logger.error("Transaction failed", {
            {"error_code", result.error_code},
            {"error_message", result.error_message}
        });
    }
}
```

#### UC-CPP-004: Connection Pool Monitoring

**User Story:** As an operations engineer, I want to monitor database connection pool health so that I can detect connection leaks and saturation.

**Scenario:**
```cpp
// connection_pool.cpp
#include <agora/log/logger.hpp>

class ConnectionPool {
public:
    Connection acquire() {
        auto logger = agora::log::get_logger("agora.portfolio.db.pool");
        
        std::unique_lock lock(mutex_);
        
        if (available_.empty()) {
            logger.warning("Connection pool exhausted, waiting", {
                {"pool_size", static_cast<int64_t>(pool_size_)},
                {"in_use", static_cast<int64_t>(in_use_)}
            });
            
            // Wait for connection with timeout
            auto timer = logger.timer("Connection wait time");
            bool acquired = cv_.wait_for(lock, std::chrono::seconds(30), 
                [this] { return !available_.empty(); });
            
            if (!acquired) {
                logger.error("Connection acquisition timeout");
                throw ConnectionPoolTimeoutException();
            }
        }
        
        auto conn = std::move(available_.back());
        available_.pop_back();
        in_use_++;
        
        AGORA_LOG_DEBUG(logger, "Connection acquired", {
            {"available", static_cast<int64_t>(available_.size())},
            {"in_use", static_cast<int64_t>(in_use_)}
        });
        
        return conn;
    }
    
    void release(Connection conn) {
        auto logger = agora::log::get_logger("agora.portfolio.db.pool");
        
        std::unique_lock lock(mutex_);
        available_.push_back(std::move(conn));
        in_use_--;
        
        AGORA_LOG_DEBUG(logger, "Connection released", {
            {"available", static_cast<int64_t>(available_.size())},
            {"in_use", static_cast<int64_t>(in_use_)}
        });
        
        cv_.notify_one();
    }
};
```

---

### Node.js NestJS Services

The JavaScript/TypeScript logging implementation is used by the Notification Service.

#### UC-JS-001: NestJS Service Initialization

**User Story:** As a Node.js developer, I want to initialize logging in my NestJS application so that all modules have access to configured loggers.

**Scenario:**
```typescript
// src/main.ts
import { NestFactory } from '@nestjs/core';
import { AppModule } from './app.module';
import { initialize, getLogger, configFromEnv, shutdown } from '@agora/logger';
import { loggingMiddleware } from '@agora/logger/express';

async function bootstrap() {
  // Initialize logging from environment
  const config = configFromEnv('notification-service');
  initialize(config);
  
  const logger = getLogger('agora.notification.main');
  logger.info('Starting Notification Service');
  
  const app = await NestFactory.create(AppModule);
  
  // Add correlation ID middleware
  app.use(loggingMiddleware());
  
  const port = process.env.PORT || 8005;
  await app.listen(port);
  
  logger.info('Service started', { port });
  
  // Graceful shutdown
  process.on('SIGTERM', async () => {
    logger.info('Received SIGTERM, shutting down');
    await app.close();
    await shutdown();
    process.exit(0);
  });
}

bootstrap();
```

#### UC-JS-002: Async Email Sending with Timing

**User Story:** As a notification service developer, I want to log email operations with timing so that I can monitor email delivery performance.

**Scenario:**
```typescript
// src/notification/notification.service.ts
import { Injectable } from '@nestjs/common';
import { getLogger, Logger } from '@agora/logger';

@Injectable()
export class NotificationService {
  private readonly logger: Logger;
  
  constructor() {
    this.logger = getLogger('agora.notification.email');
  }
  
  async sendEmail(
    userId: string, 
    templateId: string, 
    data: Record<string, unknown>
  ): Promise<SendResult> {
    const log = this.logger.withContext({
      userId,
      templateId,
      emailType: data.type
    });
    
    log.info('Preparing to send email');
    
    try {
      // Timer wraps async operation
      const result = await log.timer(
        'Email sent',
        async () => {
          const template = await this.loadTemplate(templateId);
          const rendered = this.renderTemplate(template, data);
          return await this.emailClient.send({
            to: await this.getUserEmail(userId),
            subject: rendered.subject,
            html: rendered.html
          });
        },
        { templateId } // Additional context for timer log
      );
      
      log.info('Email delivered successfully', {
        messageId: result.messageId,
        provider: result.provider
      });
      
      return result;
      
    } catch (error) {
      log.error('Failed to send email', error as Error, {
        retryable: this.isRetryableError(error)
      });
      throw error;
    }
  }
}
```

#### UC-JS-003: RabbitMQ Message Processing

**User Story:** As a developer processing async messages, I want to log message handling with correlation context so that I can trace message flows.

**Scenario:**
```typescript
// src/consumers/recommendation.consumer.ts
import { Injectable } from '@nestjs/common';
import { getLogger } from '@agora/logger';

@Injectable()
export class RecommendationConsumer {
  private readonly logger = getLogger('agora.notification.consumer.recommendation');
  
  async handleRecommendation(message: RecommendationMessage): Promise<void> {
    const log = this.logger.withContext({
      correlationId: message.correlationId,
      userId: message.userId,
      recommendationId: message.id,
      messageType: 'recommendation.created'
    });
    
    log.info('Processing recommendation notification', {
      symbol: message.symbol,
      action: message.action,
      confidence: message.confidence
    });
    
    try {
      // Check user notification preferences
      const preferences = await log.timer(
        'Loaded user preferences',
        () => this.preferencesService.get(message.userId)
      );
      
      if (!preferences.emailEnabled) {
        log.info('Email notifications disabled for user');
        return;
      }
      
      await this.notificationService.sendEmail(
        message.userId,
        'recommendation',
        {
          symbol: message.symbol,
          action: message.action,
          targetPrice: message.targetPrice
        }
      );
      
      log.info('Recommendation notification sent');
      
    } catch (error) {
      log.error('Failed to process recommendation', error as Error);
      throw error; // Will be handled by RabbitMQ retry policy
    }
  }
}
```

#### UC-JS-004: Browser Logging (React Frontend)

**User Story:** As a frontend developer, I want to log user interactions in the browser so that I can debug user-reported issues.

**Scenario:**
```typescript
// src/hooks/usePortfolioLogger.ts
import { useMemo } from 'react';
import { initializeBrowserLogger, getLogger } from '@agora/logger/browser';

// Initialize once
initializeBrowserLogger({
  serviceName: 'agora-web',
  environment: process.env.NODE_ENV as 'development' | 'production',
  console: {
    enabled: true,
    format: process.env.NODE_ENV === 'development' ? 'text' : 'json'
  },
  endpoint: '/api/logs' // Send logs to backend
});

export function usePortfolioLogger(portfolioId: string) {
  return useMemo(() => {
    return getLogger('agora.web.portfolio').withContext({ portfolioId });
  }, [portfolioId]);
}

// Usage in component
function PortfolioCard({ portfolioId }: Props) {
  const log = usePortfolioLogger(portfolioId);
  
  const handleRefresh = async () => {
    log.debug('User clicked refresh');
    
    try {
      const data = await log.timer(
        'Portfolio data loaded',
        () => fetchPortfolio(portfolioId)
      );
      
      log.info('Portfolio refreshed', {
        positionsCount: data.positions.length,
        totalValue: data.totalValue
      });
      
    } catch (error) {
      log.error('Failed to refresh portfolio', error as Error);
      showErrorToast('Failed to refresh portfolio');
    }
  };
  
  return (
    <Card>
      <Button onClick={handleRefresh}>Refresh</Button>
    </Card>
  );
}
```

---

## Context Propagation Scenarios

### CP-001: Correlation ID Propagation Across Services

**User Story:** As an operations engineer, I want to trace a request across all microservices so that I can debug issues in distributed workflows.

**Scenario:**
1. API Gateway generates correlation ID for incoming request
2. Market Data Service receives request with correlation ID in header
3. Market Data Service calls Analysis Engine via gRPC with correlation ID in metadata
4. Analysis Engine calls TimeSeries Analysis Service with correlation ID
5. All log entries across all services share the same correlation ID

**Implementation:**

```python
# Market Data Service - REST endpoint
@app.get("/analysis/{symbol}")
async def get_analysis(symbol: str):
    logger = get_request_logger()  # Has correlation_id from middleware
    
    # Call Analysis Engine via gRPC
    channel = grpc.aio.insecure_channel("analysis-engine:50053")
    stub = AnalysisServiceStub(channel)
    
    # Propagate correlation ID in gRPC metadata
    metadata = [
        ("x-correlation-id", logger.context["correlation_id"]),
        ("x-user-id", logger.context.get("user_id", ""))
    ]
    
    response = await stub.GetPrediction(
        PredictionRequest(symbol=symbol),
        metadata=metadata
    )
    
    return response
```

```cpp
// Analysis Engine - gRPC server (C++ example showing concept)
grpc::Status GetPrediction(
    grpc::ServerContext* context,
    const PredictionRequest* request,
    PredictionResponse* response
) {
    // Extract correlation ID from incoming metadata
    auto metadata = context->client_metadata();
    std::string correlation_id;
    if (auto it = metadata.find("x-correlation-id"); it != metadata.end()) {
        correlation_id = std::string(it->second.data(), it->second.size());
    }
    
    auto logger = get_logger("analysis.grpc").with_context({
        {"correlation_id", correlation_id}
    });
    
    // ... process request ...
}
```

### CP-002: User Context Tracking

**User Story:** As a compliance officer, I want all operations to be logged with user ID so that I can audit user actions.

**Scenario:**
```python
# Authentication middleware extracts user from JWT
@app.middleware("http")
async def auth_middleware(request: Request, call_next):
    token = request.headers.get("Authorization", "").replace("Bearer ", "")
    
    if token:
        try:
            payload = jwt.decode(token, SECRET_KEY)
            user_id = payload.get("sub")
            request.state.user_id = user_id
            
            # Add user to logging context
            logger = get_request_logger()
            logger.context["user_id"] = user_id
            
        except jwt.InvalidTokenError:
            pass
    
    return await call_next(request)
```

### CP-003: Hierarchical Logger Context

**User Story:** As a developer, I want child loggers to inherit parent context so that I don't have to repeat context in every log call.

**Scenario:**
```python
# Parent logger with service-level context
service_logger = get_logger("agora.market_data").with_context(
    datacenter="us-east-1",
    cluster="prod-01"
)

# Child logger inherits datacenter and cluster
api_logger = service_logger.get_child("api")

# Grandchild logger inherits all parent context
price_logger = api_logger.get_child("prices")

# This log entry includes: datacenter, cluster, and logger_name="agora.market_data.api.prices"
price_logger.info("Price updated", symbol="AAPL", price=150.25)
```

---

## Log Rotation Scenarios

### LR-001: Size-Based Rotation During High Traffic

**User Story:** As an operations engineer, I want log files to rotate automatically during high traffic periods so that disk space is managed efficiently.

**Scenario:**
```
Initial state:
  /var/log/agora/market-data.log (50MB)

High traffic period begins:
  - 1000 requests/second
  - Each request generates ~5 log entries
  - Average entry size: 500 bytes

After 20 seconds:
  /var/log/agora/market-data.log reaches 100MB
  
Rotation triggered:
  1. market-data.log.5 deleted (if exists)
  2. market-data.log.4 -> market-data.log.5
  3. market-data.log.3 -> market-data.log.4
  4. market-data.log.2 -> market-data.log.3
  5. market-data.log.1 -> market-data.log.2
  6. market-data.log -> market-data.log.1
  7. New market-data.log created
  
Rotation completes in <50ms with no lost log entries
```

### LR-002: Rotation with Concurrent Writers

**User Story:** As a developer of a multi-threaded service, I want log rotation to be thread-safe so that no log entries are lost during rotation.

**Scenario:**
```cpp
// Multiple threads writing simultaneously
void worker_thread(int thread_id) {
    auto logger = agora::log::get_logger("worker");
    
    for (int i = 0; i < 10000; i++) {
        // Thread-safe write, even during rotation
        logger.info("Processing item", {
            {"thread_id", thread_id},
            {"item_id", i}
        });
    }
}

// Rotation happens transparently
// All threads continue writing without interruption
// Write lock held only during file rename operations
```

### LR-003: Startup Recovery After Crash

**User Story:** As an operations engineer, I want the logger to handle existing log files correctly on startup so that logs are not corrupted after a crash.

**Scenario:**
```
Before crash:
  market-data.log (75MB)
  market-data.log.1 (100MB)
  market-data.log.2 (100MB)

Service crashes and restarts:

On initialization:
  1. Detect existing market-data.log
  2. Read current file size (75MB)
  3. Continue appending to existing file
  4. Rotation will occur when file reaches 100MB

No log entries lost, no manual intervention required
```

---

## Error Handling and Graceful Degradation

### EH-001: Disk Full Scenario

**User Story:** As a service owner, I want logging to continue to console when disk is full so that my service remains operational and debuggable.

**Scenario:**
```python
# Configuration with graceful degradation
config = LogConfig(
    service_name="market-data",
    console_enabled=True,
    file_enabled=True,
    file_path="/var/log/agora/market-data.log",
    # Graceful degradation enabled by default
    graceful_degradation=True
)

# During operation, disk becomes full
logger.info("Processing request")  # File write fails

# Expected behavior:
# 1. Error logged to console: "File logging failed: No space left on device"
# 2. Log entry written to console only
# 3. Application continues running
# 4. Periodic retry attempts to restore file logging
# 5. When disk space available, file logging resumes automatically
```

### EH-002: Permission Denied

**User Story:** As a developer, I want clear error messages when log files cannot be created so that I can fix configuration issues quickly.

**Scenario:**
```python
try:
    config = LogConfig(
        service_name="market-data",
        file_path="/root/logs/market-data.log"  # No permission
    )
    initialize(config)
except LogConfigError as e:
    print(f"Failed to initialize logging: {e}")
    # Output: "Failed to initialize logging: Permission denied: /root/logs/market-data.log"
    
    # Fall back to console-only logging
    config = LogConfig(
        service_name="market-data",
        file_enabled=False
    )
    initialize(config)
```

### EH-003: Async Queue Overflow

**User Story:** As a developer, I want the async logging queue to handle overflow gracefully so that my service doesn't crash during log spikes.

**Scenario:**
```python
config = LogConfig(
    service_name="market-data",
    async_enabled=True,
    queue_size=10000,  # Maximum queue size
    overflow_policy="drop_oldest"  # or "block" or "drop_newest"
)

# During extreme load, queue fills up
# With "drop_oldest" policy:
# 1. Oldest entries are dropped to make room
# 2. Warning logged: "Async queue overflow, dropped 100 oldest entries"
# 3. Recent entries preserved
# 4. Service continues operating
```

---

## Performance-Critical Logging

### PC-001: Zero-Cost Debug Logging in C++

**User Story:** As a C++ developer, I want debug logs to have zero runtime cost in production so that I can add detailed debug logging without performance impact.

**Scenario:**
```cpp
// Build with: -DAGORA_LOG_MIN_LEVEL=INFO

void process_transaction(const Transaction& txn) {
    auto logger = get_logger("transactions");
    
    // This compiles to nothing in production
    // Zero runtime overhead, not even argument evaluation
    AGORA_LOG_DEBUG(logger, "Processing transaction", {
        {"txn_id", txn.id},
        {"details", txn.serialize()}  // serialize() never called in production
    });
    
    // This always executes
    logger.info("Transaction processed", {{"txn_id", txn.id}});
}
```

### PC-002: Batched Async Writes

**User Story:** As a performance engineer, I want log writes to be batched so that I/O overhead is minimized.

**Scenario:**
```python
config = LogConfig(
    service_name="market-data",
    async_enabled=True,
    batch_size=100,        # Batch up to 100 entries
    batch_timeout_ms=10    # Or flush every 10ms
)

# Log entries are queued in memory
for symbol in symbols:
    logger.info("Processing", symbol=symbol)
    
# Background thread batches writes:
# 1. Accumulate entries until batch_size or timeout
# 2. Write batch as single I/O operation
# 3. Flush file handle once per batch
```

### PC-003: Sampling for High-Volume Logs

**User Story:** As an operations engineer, I want to sample high-volume debug logs so that I can get visibility without overwhelming storage.

**Scenario:**
```python
from agora_log import get_logger, SamplingConfig

logger = get_logger("agora.market_data.tick")

# Configure sampling for specific log patterns
logger.configure_sampling(SamplingConfig(
    # Log 1% of tick updates
    patterns=[
        {"message_contains": "tick update", "sample_rate": 0.01}
    ],
    # Always log errors
    always_log_levels=["ERROR", "CRITICAL"]
))

# Only ~1% of these will be logged
for tick in tick_stream:
    logger.debug("tick update", symbol=tick.symbol, price=tick.price)
    
# All errors are logged
logger.error("Connection lost")  # Always logged
```

---

## Distributed Tracing Integration

### DT-001: OpenTelemetry Integration

**User Story:** As a developer, I want logs to include OpenTelemetry trace IDs so that I can correlate logs with distributed traces.

**Scenario:**
```python
from opentelemetry import trace
from agora_log import get_logger
from agora_log.integrations.opentelemetry import TracingContext

logger = get_logger("agora.market_data")

@app.get("/prices/{symbol}")
async def get_price(symbol: str):
    # Get current OpenTelemetry span
    span = trace.get_current_span()
    span_context = span.get_span_context()
    
    # Create logger with tracing context
    req_logger = logger.with_context(TracingContext(
        trace_id=format(span_context.trace_id, "032x"),
        span_id=format(span_context.span_id, "016x")
    ))
    
    req_logger.info("Fetching price", symbol=symbol)
    # Log output includes trace_id and span_id for correlation with traces
```

### DT-002: Automatic Span Context Extraction

**User Story:** As a developer, I want trace context to be automatically extracted from incoming requests so that I don't have to manually propagate it.

**Scenario:**
```python
from agora_log.integrations.fastapi import LoggingMiddleware

# Middleware automatically extracts:
# - X-Correlation-ID header -> correlation_id
# - traceparent header -> trace_id, span_id
# - X-User-ID header -> user_id

app.add_middleware(LoggingMiddleware)

@app.get("/prices/{symbol}")
async def get_price(symbol: str):
    logger = get_request_logger()
    # Logger automatically has correlation_id, trace_id, span_id
    logger.info("Processing request", symbol=symbol)
```

### DT-003: Log-to-Trace Linking in Grafana

**User Story:** As an operations engineer, I want to click on a log entry in Grafana and see the corresponding trace so that I can investigate issues quickly.

**Scenario:**
```
Log entry in Loki:
{
  "timestamp": "2025-01-15T10:30:45.123Z",
  "level": "ERROR",
  "message": "Database query timeout",
  "trace_id": "abc123def456789...",
  "span_id": "span789...",
  "service": "market-data-service"
}

In Grafana:
1. View log entry in Loki dashboard
2. Click "View Trace" button
3. Automatically navigate to Jaeger/Tempo with trace_id
4. See full distributed trace showing where timeout occurred
```

---

## Summary

This document covers the primary use cases for the Agora logging library across:

- **Python FastAPI services** - Market Data, Analysis Engine, Recommendations, Time Series
- **C++ gRPC services** - Portfolio Manager with ultra-low latency requirements
- **Node.js NestJS services** - Notification Service with async operations
- **Browser applications** - React frontend with user interaction logging

Key scenarios addressed:
- Service initialization and configuration
- Request-scoped logging with correlation IDs
- Operation timing and performance monitoring
- Batch processing and progress logging
- Cross-service context propagation
- File rotation and disk management
- Error handling and graceful degradation
- Performance optimization techniques
- Distributed tracing integration

Each use case includes concrete code examples that can be used as reference implementations.
